{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Path to the main dataset directory\n",
    "source_dir = r\"C:\\Users\\user\\Downloads\\mosquito\"\n",
    "output_dir = r\"C:\\Users\\user\\Downloads\\sorted_dataset\"\n",
    "\n",
    "# Folder names\n",
    "categories = [\"Aedes aegypti\", \"Anopheles gambiae\", \"Culex pipiens\", \"Haemagogus janthinomys\", \"Sabethes cyaneus\"]\n",
    "train_ratio = 0.8  # 80% for training, 20% for validation\n",
    "\n",
    "# Create output folders\n",
    "train_dir = os.path.join(output_dir, \"train\")\n",
    "validation_dir = os.path.join(output_dir, \"validation\")\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(validation_dir, exist_ok=True)\n",
    "\n",
    "# Function to create subfolders\n",
    "def create_subfolders(base_dir, categories):\n",
    "    for category in categories:\n",
    "        os.makedirs(os.path.join(base_dir, category), exist_ok=True)\n",
    "\n",
    "# Create subfolders for each category\n",
    "create_subfolders(train_dir, categories)\n",
    "create_subfolders(validation_dir, categories)\n",
    "\n",
    "# Sort files into train and validation folders\n",
    "for category in categories:\n",
    "    category_dir = os.path.join(source_dir, category)\n",
    "    if not os.path.exists(category_dir):\n",
    "        print(f\"Category folder not found: {category_dir}\")\n",
    "        continue\n",
    "\n",
    "    files = os.listdir(category_dir)\n",
    "    random.shuffle(files)  # Shuffle for random distribution\n",
    "\n",
    "    # Split files into training and validation sets\n",
    "    split_index = int(len(files) * train_ratio)\n",
    "    train_files = files[:split_index]\n",
    "    validation_files = files[split_index:]\n",
    "\n",
    "    # Move files to train folder\n",
    "    for file in train_files:\n",
    "        src_path = os.path.join(category_dir, file)\n",
    "        dest_path = os.path.join(train_dir, category, file)\n",
    "        shutil.copy2(src_path, dest_path)\n",
    "\n",
    "    # Move files to validation folder\n",
    "    for file in validation_files:\n",
    "        src_path = os.path.join(category_dir, file)\n",
    "        dest_path = os.path.join(validation_dir, category, file)\n",
    "        shutil.copy2(src_path, dest_path)\n",
    "\n",
    "print(\"Dataset sorting completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Paths to the dataset\n",
    "base_dir = r\"C:\\Users\\user\\Downloads\\sorted_dataset\"\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "validation_dir = os.path.join(base_dir, \"validation\")\n",
    "\n",
    "# Image parameters\n",
    "IMG_HEIGHT = 150\n",
    "IMG_WIDTH = 150\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 5  # Number of mosquito species\n",
    "\n",
    "# Create Image Data Generators\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "train_data = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_data = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Build the CNN Model\n",
    "def build_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "model = build_model()\n",
    "\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    epochs=10,\n",
    "    validation_data=validation_data\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model_path = \"mosquito_species_model.h5\"\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Plot Training and Validation Accuracy/Loss\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history)\n",
    "\n",
    "# Predict Function\n",
    "def predict_image(image_path, model):\n",
    "    img = Image.open(image_path).resize((IMG_WIDTH, IMG_HEIGHT))\n",
    "    img_array = np.array(img) / 255.0  # Normalize the image\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "    class_indices = train_data.class_indices\n",
    "    class_names = list(class_indices.keys())\n",
    "    \n",
    "    predicted_class = class_names[np.argmax(predictions)]\n",
    "    confidence = np.max(predictions) * 100\n",
    "\n",
    "    print(f\"Predicted Species: {predicted_class} ({confidence:.2f}%)\")\n",
    "\n",
    "# Test the Model with a New Image\n",
    "test_image_path = r\"C:\\Users\\user\\Downloads\\test\\download.jpeg\"  # Replace with your image path\n",
    "if os.path.exists(test_image_path):\n",
    "    loaded_model = load_model(model_path)\n",
    "    predict_image(test_image_path, loaded_model)\n",
    "else:\n",
    "    print(\"Test image not found. Please check the file path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
